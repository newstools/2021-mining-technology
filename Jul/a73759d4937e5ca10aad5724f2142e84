If this is the era of fake news, then it’s likely to be the calm before the storm. The true tempest will begin to howl once the age of deepfakes commences. Should uncannily faked video content overlap with a new pandemic or increased cyber warfare, then we’ll be in line for a more monstrous kind of misinformation. The EU has already proposed regulation artificial intelligence (AI) which will require organisations to disclose any deepfake usage and creation. No doubt this has been spurred on by recent events. As a GlobaData report on misinformation states, the Covid-19 pandemic has been “fertile ground for fake news and the exploitation of public fear.” Social media platforms have struggled to contain the spread of hoaxes, conspiracy theories and quack cures, with some reviewing their ad policy to forbid the promotion of medical misinformation. Despite these efforts, they have found themselves being grilled by politicians about their failure to prevent the spread of misinformation on their platforms. The debate grew particularly contentious following the deadly riots on Capitol Hill on 6 January after a mismatched mob of Donald Trump supporters, QAnon conspiracists and anti-maskers were egged on by fake news that the US presidential election had been stolen. However, most problems aren’t caused by malicious “maskholes”, but by people impulsively sharing information without thinking critically about it. This poses some questions. Can audiences be educated to think more before posting? How can fake news be properly regulated? Verdict finds out with Henry Brown, data and analytics director at Ciklum; Jared Ficklin, chief creative technologist at argodesign; Andy Parsons, director of Content Authenticity Initiative at Adobe; Andy Patel, researcher, AI Center of Excellence at F-Secure; and Rachel Roumeliotis, vice president of data and AI at O’Reilly. Through separate discussions with these experts we investigate whether AI has any part to play in proceedings, and how much social media platforms can realistically solve the issue. The global pandemic is likely to be a point of no return for big social brands as, after the crisis, they will be expected to keep policing harmful content. Regulating fake news would impact on advertisers, their clients and any platform that carries content and advertising. Additionally, it would have a positive effect for businesses which may find themselves the target of misinformation for being too large and/or if involved with controversial public figures and practices. Andy Patel, F-Secure: Misinformation and cult-like behaviour exists on all sides of the political equation. Examples from the left include the divides seen between [Black Lives Matter], Bernie Sanders, and Hillary Clinton supporters during the 2016 US elections and, more recently, in-fighting between supporters of left-leaning political parties and figures in the UK. The misinformation shared by these groups is used to further infighting and has been quite specific to each group’s political agenda. As such, this misinformation hasn’t had a notable impact on the general public. QAnon, anti-vax, anti-mask, ‘Covid hoax’ and ‘Stop the Steal’ narratives have all impacted the general public, and that’s why there’s been focus on misinformation coming from the right. Note, however, that any misinformation shared by any of these groups – whether they’re on the left or right – can be leveraged by adversaries – both domestic and foreign – to cause further harm to society. Jared Ficklin, argodesign: We most often hold up manipulating ‘facts’ to gain political power as the key motivator of fake news. But there are a lot of other motivators like selling ads using targeted affiliate marketing that motivate one to write fake news. In reality, ad revenue probably motivates fake news to a much broader extent. Agreement bias is a powerful psychology being exploited here. If someone believes the Earth is flat, they are more likely to read stories about a flat Earth. They also are more likely to trust the advertisers in that story if it agrees with their point of view. So if you want to sell a Flat Earth T-shirt, you should write a story about a Flat Earth society and then use targeted social media to put it in front of flat Earth Believers. This is only by the slightest technicality journalism, and before social media would be the domain of a zine or brochure. It would never run in a paper. The audience is too narrow.  Jared Ficklin, chief creative technologist at argodesign Henry Brown, Ciklum: Natural language processing  (NLP) can be used to detect nuances in grammar, spelling and sentence structure, which in turn may reveal an issue with the original author of an article or piece of content. Network techniques may also help to detect particular users on social media platforms who are more prone to sharing fake news, and thus try to encourage better enforcement of the content that they share. A fake news warning could then be shared with other users on the platform. Ficklin: The Real News Layer (a project I’m exploring) is designed to amplify our critical thinking skills. It’s a combination of a well-designed user interface and traditional pattern-matching algorithms and technologies of spidering the web to create a data pool. AI can enhance these with much better matching in order to better collate and correlate stories for us. It isn’t a fact checker, it is a researcher. Where traditional algorithms would fail to identify two stories being on the same subject, AI can be deployed to determine nuance. Ficklin: Perhaps the system could be primed with the very few things that can be pure fact, such as the force of gravity on Earth, but in many ways it should not be told what is truth. It should instead be a comparator of what is written and by whom and how close they are to the source or actual event. Let’s say there is a Supreme Court ruling you are reading about. The Real News Layer could mention to you that it has found 100 variations of the article you are reading about this specific judgement on this specific date. Out of the variations there are a total of 50 facts asserted and your article only contains four of those assertions, which is way off the average. It could reveal your story also includes unique assertions. It could also reveal that your article is way down the attribution tree. When you take a quick look at this you realise your article only reveals the opinions of the two dissenting justices of a certain political party and is actually written based on another article which was based on another article. Meanwhile you can see much of the rest of the world is reading an article with more information in it written by a reporter who was actually at the hearing. Ficklin: It is at that point you could look at the reporter for your article. The AI has ranked this reporter for placement and attribution. They seem to attribute a lot of others but don’t ever get attributed. Further, their stories seem to only appear in one publication that is read by only a specific cohort of your friends. Andy Parsons, Adobe: I do think that’s sometimes the case, but given Adobe’s relationship with the creative community we’ve created the Content Authenticity Initiative (CAI) coalition to address their needs and acknowledge the crucial role they play. With the rise of completely synthetic visual objects, including human depictions, deceptive content can be created without editing prior content. This is why we must re-establish a common understanding of objective facts, before our understanding of what’s ‘real’ online is eroded beyond recovery. Andy Parsons, director of Content Authenticity Initiative at Adobe Parsons: The core ideas behind the CAI work apply equally well to any type of media, including text, audio, images and video. It is essential to develop techniques and standards to combat inauthentic content now for two reasons. First, we have seen extremely dangerous examples of disinformation that deliberately mislead with simple, unsophisticated attacks. Bad actors won’t necessarily reach for deepfakes if their goals can be achieved with simpler means. Second, detection of deepfakes is an arms race that is already underway. Creators of malicious content utilise ever more sophisticated tools and the detection algorithms have to keep pace. Given that tools for making Hollywood-quality synthetics are not only more approachable now, but inexpensive or free, it’s more critical than ever to have measures in place for good actors to use these tools responsibly, and for consumers to have access to information about how their content came to be. Parsons: We expect CAI provenance data to be an important training signal for AI detection models, and for provenance to be used in concert with detection results. For instance, a detector algorithm might use AI to score an image as 80% likely to be authentic then consult the image’s provenance before delivering an assessment. And the inverse is also important: Our provenance can include the results of AI detection as part of its sealed, verifiable data. For example, if a detection algorithm were used on a video uploaded to social media, the results of the analysis could be attached to the video in the same way verifiable assertions like copyright, camera data and edit history are secured. With this exposed transparently, downstream platforms and consumers can then decide whether the video can be trusted. Rachel Roumeliotis, O’Reilly: With the advent of organisations like the CAI and non-fungible tokens (NFTs) taking somewhat centre stage, creators digitally stamping their work will gain momentum over the next few years. What might be another interesting question to think about is what if AI creates something? Who owns that? Who does that track back to :  the algorithm writer, a corporation? As technologies like NLP are already doing this, it’s something we’ll need to address in the near future. Brown: Deepfakes can be created by AI ,  but AI can also be used to detect it. In fact, the ability of artificial intelligence (specifically Generative Adversarial Networks or GANs) to create fake content is directly related to its ability to detect it. AI will learn to create better examples of deepfakes if it can differentiate between real and fictitious. One machine learning (ML) solution generates content (the generator), and another ML solution (the discriminator) tries to detect whether the content is a fake or not. Of course, a group that wants to create and release fake news, generated by GANs, probably wouldn’t release their discriminator algorithms, but at least in theory someone else could build their own discriminating machine learning solution to tackle it. AI can also use ‘fingerprinting’ techniques to determine whether an image predates the claimed event or moment.  Rachel Roumeliotis, vice president of data and AI at O’Reilly Ficklin: This is an area where using AI for establishing that two stories are actually about the same thing is important. (For example) was Pizzagate thrown in to trigger an agreement bias? Patel: Researchers and journalists warned us about the dangers of QAnon and “Stop the Steal” a long time before social network sites took action. Many of the same people predicted something would happen on January 6 2021. In order to stop online cults escalating out of control, social networks need to listen a lot more closely to what these researchers and journalists are saying and take timely and appropriate actions. Social networks have implemented automation that effectively prevents ISIS-related content and accounts from persisting on their websites. They also often publish reports and datasets related to a wide range of adversarial campaigns detected on their platforms (in different languages, from different geographical regions). Patel: When it comes to moderation of disinformation and online harassment, non-English-speaking (and indeed non-US) regions are left behind. This problem could be solved by setting up communication mechanisms between social networks and researchers/policy groups in non-US countries. Academic research efforts into detection of disinformation, hate speech and online harassment largely focus on English language examples. As such, English language training data is much more plentiful and of higher quality. The methodologies being developed are applicable to any language: they just need access to good datasets. Roumeliotis: Companies like Facebook have processes in place and indicators of where something has come from, but people should always try to find news at its source. If it does seem outlandish or strange, cross-reference it with another trusted news source to weigh its validity. Brown: Whilst prevention of misinformation spread is absolutely key, it comes down to the end-users: who must learn and choose not to engage with fake content online. This in itself requires teaching, as well as the provision of tools that will help people to identify what is fake, and what isn’t. Even with these tools in place, if people continue to choose to engage with misinformation, then there really is only so much that AI can do. Ficklin: Someone who believes the Earth is flat is past the point of responding to fact checking. Instead it is better to offer them a contextualised view. Let them know, using simple UI, that the story they’re reading is one out of 1 billion stories about the shape of the Earth and the other 999,999,999 say the Earth is round. Tell them that only people who believe the Earth is flat are reading the story in front of them, that the video in the story is two seconds out of a longer video about the round Earth, that the story is actually 10 years old and there are newer versions you can read. By Verdict’s Giacomo Lee. Find GlobalData’s Thematic Research Misinformation report here. Latest report from 

  
	Browse over 50,000 other reports on our store.
  

  Visit GlobalData Store Mining Contractors and Engineers Heavy-Duty Mechanical Equipment for Mines Conveyor Belt Maintenance, Wear and Corrosion Protection Mining Contractors and Engineers Heavy-Duty Mechanical Equipment for Mines Conveyor Belt Maintenance, Wear and Corrosion Protection